{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Machine Learning Algorithms Applied to the Lens Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Authors: Jenny Kim (jennykim1016), Ji Won Park (jiwoncpark)\n",
    "\n",
    "In this notebook, we apply classifical machine learning algorithms such as linear SVC, nearest neighbor, and random forest to the problem of classifying lenses vs. non-lenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys, os\n",
    "realizer_path = os.path.join(os.environ['SLREALIZERDIR'], 'slrealizer')\n",
    "sys.path.insert(0, realizer_path)\n",
    "from utils.utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.environ['SLREALIZERDIR'], 'data')\n",
    "\n",
    "lens_object_f = os.path.join(data_path, 'lens_object_table.csv')\n",
    "nonlens_object_f = os.path.join(data_path, 'nonlens_object_table.csv')\n",
    "\n",
    "lens_obj = pd.read_csv(lens_object_f)\n",
    "num_data = len(lens_obj)\n",
    "nonlens_obj = pd.read_csv(nonlens_object_f).query('(u_trace < 5.12)').sample(num_data, random_state=123).reset_index(drop=True)\n",
    "assert len(lens_obj) == len(nonlens_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Make the feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the cornerplot that we drew of SDSS and OM10 (see notebook `Comparing+OM10+vs+SDSS+Objects`), we idenfity features that seem to most strongly differ between lenses and non-lenses. We will hand-engineer the following six features:\n",
    "\n",
    "- Difference in sizes between u and z bands\n",
    "- Difference in ellipticities between u and z bands (e)\n",
    "- Difference in rotation angles of the systems between u and z bands (ϕ)\n",
    "- Difference in angles (ω) between centroid positions and galactic shears \n",
    "- Difference in magnitudes between u and z bands\n",
    "- Difference in positions of the centroid between u and z bands (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [lens_obj, nonlens_obj]:\n",
    "    for b in 'uz':\n",
    "        df[b + '_e'], df[b + '_phi'] = e1e2_to_ephi(e1=df[b + '_e1'], e2=df[b + '_e2'])\n",
    "        df[b + '_mag'] = from_flux_to_mag(lens_obj[b + '_apFlux'], from_unit='nMgy')\n",
    "        df[b + '_mag'][~np.isfinite(df[b + '_mag'])] = 100.0\n",
    "        df[b + '_posmod'] = np.power(np.power(df[b + '_x'], 2.0) + np.power(df[b + '_y'], 2.0), 0.5)\n",
    "        df[b + '_omega'] = (df[b + '_e1']*df[b + '_x'] + df[b + '_e2']*df[b + '_y'])/(df[b + '_e']*df[b + '_posmod'])\n",
    "\n",
    "for df in [lens_obj, nonlens_obj]:\n",
    "    df['delta_pos'] = np.power(np.power(df['u_x'] - df['z_x'], 2.0) + np.power(df['u_y'] - df['z_y'], 2.0), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_truth_table(df, attributes, truth_value, save_file=None):\n",
    "    num_attributes = len(attributes)\n",
    "    num_data = len(df)\n",
    "    #features = np.empty((num_features, num_attributes))\n",
    "    features_dict = {}\n",
    "    col_names = ['delta_' + a for a in attributes] + ['label']\n",
    "    \n",
    "    for a in attributes:\n",
    "        if a == 'pos':\n",
    "            features_dict['delta_' + a] = df['delta_' + a]\n",
    "        else:\n",
    "            features_dict['delta_' + a] = df['u_' + a] - df['z_' + a]\n",
    "        \n",
    "    features_dict['label'] = np.ones((num_data, ))*truth_value\n",
    "    #features = np.array(features_dict.values()).reshape(num_data, num_attributes + 1)\n",
    "    data = pd.DataFrame.from_dict(features_dict)\n",
    "    data = data[col_names]\n",
    "    if save_file is not None:\n",
    "        data.to_csv(save_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['trace', 'e', 'phi', 'mag', 'pos', 'omega']\n",
    "lens_data = make_truth_table(df=lens_obj, attributes=attributes, truth_value=1)\n",
    "nonlens_data = make_truth_table(df=nonlens_obj, attributes=attributes, truth_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(lens_data.shape, nonlens_data.shape)\n",
    "total_data = pd.concat([lens_data, nonlens_data], axis=0)\n",
    "print(total_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning + Precision Recall Curve for various methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going two use three different algorithms : linearSVC, K-neighbors, and Random Forest. For the K-neighbors and Random Forest, we are going to change the number of neighbors and leaves. Then, we will see which classifier has the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do so, we import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the entire dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = total_data.values\n",
    "#print(total_data_arr.shape)\n",
    "y = total_data[:, -1]\n",
    "X = total_data[:, :-1]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=123, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just making sure the train and test sets have an even number of positive and negative examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of positive examples in training set: %0.2f\" \n",
    "      %(len(y_train[y_train==1])/float(len(y_train))),\n",
    "      \"\\n ... in test set: %0.2f\" \n",
    "      %(len(y_test[y_test==1])/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Precision-recall curves (PRCs) for various methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define `models_dict`, a dictionary of models to use. The `colors_dict` assigns a color to each model for plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {'svm': CalibratedClassifierCV(svm.LinearSVC()),\n",
    "               'nn3': KNeighborsClassifier(n_neighbors=3),\n",
    "               'nn5': KNeighborsClassifier(n_neighbors=5),\n",
    "               'rf3': RandomForestClassifier(n_estimators=3),\n",
    "               'rf5': RandomForestClassifier(n_estimators=5),\n",
    "               'rf10': RandomForestClassifier(n_estimators=10),\n",
    "              }\n",
    "\n",
    "colors_dict = {'svm': 'red',\n",
    "               'nn3': 'orange',\n",
    "               'nn5': 'green',\n",
    "               'rf3': 'blue',\n",
    "               'rf5': 'purple',\n",
    "               'rf10': 'black'\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, model in models_dict.iteritems():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_score = model.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    plt.plot(recall, precision, label=label, color=colors_dict[label])\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.title('Precision-recall curves (PRCs) for various methods')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Receiver operating characteristic (ROC) curve for various methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going two use three different algorithms : linearSVC, K-neighbors, and Random Forest. For the K-neighbors and Random Forest, we are going to change the number of neighbors and leaves. Then, we will see which classifier has the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label, model in models_dict.iteritems():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_score = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=label + \" area = %0.2f\" %roc_auc, color=colors_dict[label])\n",
    "\n",
    "plt.xlabel('False positive rate (FPR)')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.plot([-0.1, 1.1], [-0.1, 1.1], 'k--')\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.title('Receiver operating characteristic (ROC) curves for various methods')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We could see that the random forest classifier with `N=10` and above performed the best. Because Random Forest can also give the measures of how the useful each feature was, we draw a histogram of feature importance. For this purpose, we will use the ExtraTreesClassifier model instead of the original Random Forest with 10 neighbors. Plotting instructions taken from this [page](http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) of the scikit-learn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=123)\n",
    "\n",
    "forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_names = ['delta_' + a for a in attributes]\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "col_names_sorted = [col_names[o] for o in indices]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, col_names_sorted[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "# Black vertical lines represent inter-trees variability\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), col_names_sorted, rotation='vertical')\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
